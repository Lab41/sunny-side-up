{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import scipy.stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import os, sys, getopt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "Found 4 items\n",
      "drwxr-xr-x   - ytesfaye ytesfaye          0 2015-05-01 17:11 /user/ytesfaye/twitterParquet/april.parquet\n",
      "drwxr-xr-x   - ytesfaye ytesfaye          0 2015-05-01 00:31 /user/ytesfaye/twitterParquet/march.parquet\n",
      "drwxr-xr-x   - ytesfaye ytesfaye          0 2015-06-02 16:45 /user/ytesfaye/twitterParquet/may.parquet\n",
      "drwxr-xr-x   - ytesfaye ytesfaye          0 2015-06-02 16:51 /user/ytesfaye/twitterParquet/month=4.parquet\n"
     ]
    }
   ],
   "source": [
    "# verify spark binary in HDFS\n",
    "!hadoop fs -ls /FIXME/twitterParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark-1.4.1-bin-hadoop2.4/python/pyspark/sql/context.py:384: UserWarning: parquetFile is deprecated. Use read.parquet() instead.\n",
      "  warnings.warn(\"parquetFile is deprecated. Use read.parquet() instead.\")\n"
     ]
    }
   ],
   "source": [
    "# load data and register a table\n",
    "tweets = sqlCtx.parquetFile('hdfs://cdh:8020/FIXME/twitterParquet/may.parquet/part-r-00001.parquet')\n",
    "tweets.registerTempTable('tweets')\n",
    "tweets.count()\n",
    "#tweets = sqlCtx.parquetFile('hdfs://cdh:8020/FIXME/twitterParquet/may.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure utf8 encoding\n",
    "import sys    \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select arabic tweets that are non-retweets \n",
    "arabic_tweets = sqlCtx.sql(\"select id_str, text from tweets where lang = 'ar' AND retweet_count = '0' AND text NOT LIKE 'RT @%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3919"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the number of filtered results\n",
    "arabic_tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mimic `mkdir --parents`\n",
    "def mkdir_p(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "# mimic `chmod -R`\n",
    "def chmod_recursive(directory, perms):\n",
    "    for root, dirs, files in os.walk(directory):  \n",
    "        \n",
    "        # process all files and directories at once\n",
    "        dirs.extend(files)\n",
    "        \n",
    "        # traverse and update\n",
    "        for pathname in dirs:  \n",
    "            os.chmod(os.path.join(root, pathname), perms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output\n",
    "dir_root = '/ipython/arabic'\n",
    "dir_files = 'may_part-r-00001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ensure paths exist\n",
    "mkdir_p(dir_root)\n",
    "mkdir_p(os.path.join(dir_root, dir_files))\n",
    "chmod_recursive(dir_root, 0777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build filename to include date\n",
    "filename = '{}/{}_{}.csv'.format(dir_root, dir_files, time.strftime(\"%Y.%m.%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert spark->pandas dataframe to save locally\n",
    "df = arabic_tweets.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save 'text' column in each row of dataframe as dir_root/dir_files/<tweet-id>.txt\n",
    "def save_tweet_as_text(row, field=\"text\", minlen=45):\n",
    "    \n",
    "    # format the filename\n",
    "    filename = '{}/{}/{}.text'.format(dir_root, dir_files, row['id_str'])\n",
    "    \n",
    "    # only save longer tweets\n",
    "    if (len(row[field]) >= minlen):\n",
    "        with open(filename, \"w\") as text_file:\n",
    "            text_file.write(\"{}\".format(row[field]))\n",
    "        \n",
    "    # return the filename to the calling method\n",
    "    return filename\n",
    "\n",
    "\n",
    "# compress source directory into an output filename.tar.gz\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save each tweet as an individual text file\n",
    "df.apply(lambda row: save_tweet_as_text(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an archive of all tweets\n",
    "make_tarfile('{}/{}.tar.gz'.format(dir_root, dir_files), '{}/{}'.format(dir_root, dir_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
